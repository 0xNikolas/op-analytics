{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas web3 hexbytes rlp fastlz clickhouse-connect\n",
    "# ! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme\n",
    "FastLZ needs Python version 3.9x or lower, make sure your environment is using a later python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from hexbytes import HexBytes\n",
    "import ast\n",
    "import rlp\n",
    "from rlp.sedes import Binary, big_endian_int, binary, List\n",
    "from eth_utils import to_bytes, to_hex\n",
    "import fastlz\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "sys.path.pop()\n",
    "\n",
    "client = ch.connect_to_clickhouse_db() #Default is OPLabs DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run configs\n",
    "schemas_to_select = [\n",
    "        # 'op', \n",
    "        # 'base',\n",
    "        # 'mode',\n",
    "        'fraxtal',\n",
    "        # 'zora'\n",
    "        ]  # Add more schemas as needed\n",
    "days_of_data = 28\n",
    "\n",
    "#FastLZ Regression Metrics\n",
    "# Specs - https://specs.optimism.io/fjord/exec-engine.html?search=#fjord-l1-cost-fee-changes-fastlz-estimator\n",
    "intercept = -42_585_600\n",
    "fastlzCoef = 836_500\n",
    "minTransactionSize = 100\n",
    "scaled_by = 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "csv_path = '../../op_chains_tracking/outputs/chain_metadata.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter the DataFrame based on the schemas_to_select list\n",
    "filtered_df = df[df['oplabs_db_schema'].isin(schemas_to_select)]\n",
    "\n",
    "# Select the required columns and convert to a list of dictionaries\n",
    "chain_mappings_list = filtered_df[['oplabs_db_schema', 'display_name', 'mainnet_chain_id']].rename(\n",
    "    columns={'oplabs_db_schema': 'schema_name', 'mainnet_chain_id': 'chain_id'}\n",
    ").to_dict(orient='records')\n",
    "\n",
    "# Print the resulting list of dictionaries\n",
    "print(chain_mappings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test transaction receipt\n",
    "# from web3 import Web3\n",
    "# op_rpc = os.getenv(\"OP_PUBLIC_RPC\")\n",
    "# w3 = Web3(Web3.HTTPProvider(op_rpc))\n",
    "\n",
    "# tx_test = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "# tx = w3.eth.get_transaction(tx_test)\n",
    "# txr = w3.eth.get_transaction_receipt(tx_test)\n",
    "# # # txraw = w3.eth.get_raw_transaction(tx_test)\n",
    "# print(tx)\n",
    "# # print(txr)\n",
    "# # # print(txraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may not sufficent due to missing transaction signature fields\n",
    "\n",
    "# Get L2 Txs from Clickhouse / Goldsky\n",
    "query_by_day = '''\n",
    "        SELECT @chain_id@ as chain_id, nonce, gas, max_fee_per_gas, max_priority_fee_per_gas,\n",
    "                to_address as to, value, input, block_timestamp, block_number, hash, receipt_gas_used\n",
    "        FROM @chain_db_name@_transactions\n",
    "        WHERE gas_price > 0\n",
    "        # 1 day chunk\n",
    "        AND block_timestamp < DATE_TRUNC('day',NOW()) - interval '@day_num@ days'\n",
    "        AND block_timestamp >= DATE_TRUNC('day',NOW()) - (interval '@day_num@ days') - (interval '1 day')\n",
    "\n",
    "        SETTINGS max_execution_time = 3000\n",
    "'''\n",
    "# AND hash = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "# AND block_number = 120731426\n",
    "\n",
    "# txs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process transactions and RLP encode\n",
    "#https://ethereum.org/en/developers/docs/transactions/\n",
    "\n",
    "# NOTE THE RLP ENCODING IS NOT 1:1 WITH ETHERSCAN YET (but it's ~close-ish)\n",
    "def process_and_encode_transaction(row):\n",
    "    to_hexstr = 'eee'\n",
    "    try:\n",
    "        # Check if \"to\" field is None or empty\n",
    "        to_field = row[\"to\"]\n",
    "        if isinstance(to_field, bytes) and to_field == b'\\x00' * len(to_field):\n",
    "            to_hexstr = b''\n",
    "        else:\n",
    "            try:\n",
    "                to_hexstr = to_bytes(hexstr=to_field.decode('utf-8')) if to_field is not None else b''\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(f\"Problematic byte sequence: {to_field[e.start:e.end]}\")\n",
    "        tx_params = {\n",
    "            'chainId': int(row['chain_id']),\n",
    "            'nonce': row['nonce'],\n",
    "            'maxPriorityFeePerGas': row['max_priority_fee_per_gas'],\n",
    "            'maxFeePerGas': row['max_fee_per_gas'],\n",
    "            'gas': row['gas'],\n",
    "            'to': to_hexstr,\n",
    "            'value': to_bytes(row['value']),\n",
    "            'input': HexBytes(row['input']),\n",
    "            'accessList': row['access_list'],  # This is assumed to be an empty list or properly formatted\n",
    "            'v': row['v'],\n",
    "            'r': HexBytes(row['r']),\n",
    "            's': HexBytes(row['s'])\n",
    "        }\n",
    "\n",
    "        transaction = [\n",
    "            # tx_params['chainId'],\n",
    "            # tx_params['nonce'],\n",
    "            # tx_params['gas'],\n",
    "            # tx_params['maxFeePerGas'],\n",
    "            # tx_params['maxPriorityFeePerGas'],\n",
    "            # tx_params['to'],\n",
    "            # tx_params['value'],\n",
    "            # tx_params['input'],\n",
    "            # tx_params['accessList'],\n",
    "            # tx_params['v'],\n",
    "            # tx_params['r'],\n",
    "            # tx_params['s']\n",
    "            # https://goethereumbook.org/en/transaction-raw-create/\n",
    "            tx_params['nonce'],\n",
    "            tx_params['to'],\n",
    "            tx_params['value'],\n",
    "            tx_params['gas'],\n",
    "            tx_params['maxFeePerGas'],\n",
    "            tx_params['input'],\n",
    "            tx_params['chainId'],\n",
    "            tx_params['v'],\n",
    "            tx_params['r'],\n",
    "            tx_params['s']\n",
    "        ]\n",
    "\n",
    "        encoded_tx = rlp.encode(transaction)\n",
    "        return Web3.to_hex(encoded_tx), len(encoded_tx)\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Failed Transaction Info:\")\n",
    "        print(row)\n",
    "        print(to_hexstr)\n",
    "        return None, None\n",
    "\n",
    "# Function to compress transaction data\n",
    "def compress_transaction(encoded_transaction):\n",
    "\n",
    "    hex_string = encoded_transaction[2:]\n",
    "    # Convert the hexadecimal string to bytes\n",
    "    byte_string = bytes.fromhex(hex_string)\n",
    "    compressed_data = fastlz.compress(byte_string)\n",
    "\n",
    "    return compressed_data.hex(), len(compressed_data)\n",
    "# Define a function to apply to each row of the DataFrame\n",
    "def process_and_compress_transaction(row):\n",
    "    encoded_tx = row['encoded_transaction']\n",
    "    compressed_tx, len_tx = compress_transaction(encoded_tx)\n",
    "    return compressed_tx, len_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for chain in chain_mappings_list:\n",
    "        for day_num in range(0,days_of_data):\n",
    "                print(chain['schema_name'] + ' : day ' + str(day_num))\n",
    "                query_map = query_by_day\n",
    "\n",
    "                query_map = query_map.replace(\"@chain_db_name@\", chain['schema_name'])\n",
    "                query_map = query_map.replace(\"@chain_id@\", str(chain['chain_id']))\n",
    "                query_map = query_map.replace(\"@day_num@\", str(day_num))\n",
    "                \n",
    "                query_start_time = time.time()\n",
    "                result_df = client.query_df(query_map)\n",
    "                query_end_time = time.time()  # Record the start time\n",
    "                query_elapsed_time = query_end_time - query_start_time\n",
    "                print (f\"        Query Done: Completed in {query_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Add Dummy Signature and fields\n",
    "                result_df['access_list'] = '[]'\n",
    "                result_df['access_list'] = result_df['access_list'].apply(ast.literal_eval)\n",
    "                result_df['r'] = '0x6727a53c0972c55923242cea052dc4e1105d7b65c91c442e2741440965eac357'\n",
    "                result_df['s'] = '0x0a8e71aea623adb7b5562fb9a779634f3b84dad7be1e1f22caaa640db352a6ff'\n",
    "                result_df['v'] = '55'\n",
    "\n",
    "                # Assuming `txs_df` is your DataFrame\n",
    "                result_df[['encoded_transaction', 'len_encoded_transaction']] = result_df.apply(process_and_encode_transaction, axis=1, result_type='expand')\n",
    "                enc_end_time = time.time()  # Record the start time\n",
    "                enc_elapsed_time = enc_end_time - query_end_time\n",
    "                print (f\"        Encoding Done: Completed in {enc_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Apply compression to each transaction in the DataFrame\n",
    "                result_df[['compressed_transaction', 'compressed_transaction_length']] = result_df.apply(process_and_compress_transaction, axis=1, result_type='expand')\n",
    "                comp_end_time = time.time()\n",
    "                comp_elapsed_time = comp_end_time - enc_end_time\n",
    "                print (f\"        Compression Done: Completed in {enc_elapsed_time:.2f} seconds\")\n",
    "                \n",
    "                # Calculate estimated size for each row\n",
    "                result_df['estimatedSize_raw'] = result_df.apply(lambda row: (intercept + (row['compressed_transaction_length'] * fastlzCoef)) / scaled_by, axis=1)\n",
    "                # Calculate minimum value for 'estimatedSize' column\n",
    "                result_df['estimatedSize'] = result_df.apply(lambda row: max(minTransactionSize, row['estimatedSize_raw']), axis=1)\n",
    "                est_end_time = time.time()\n",
    "                est_elapsed_time = est_end_time - comp_end_time\n",
    "                print (f\"        Estimation Done: Completed in {est_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Agg L2\n",
    "                # Convert block_timestamp to date (truncate to day)\n",
    "                result_df['block_date'] = pd.to_datetime(result_df['block_timestamp']).dt.date\n",
    "                grouped_df = result_df.groupby(['block_date', 'chain_id'])\n",
    "                # Define aggregation functions\n",
    "                agg_functions = {\n",
    "                        'len_encoded_transaction': ['sum', 'mean', 'count'],\n",
    "                        'estimatedSize': ['sum', 'mean']\n",
    "                }\n",
    "                # Perform aggregation\n",
    "                aggregated_df = grouped_df.agg(agg_functions).reset_index()\n",
    "                # Rename columns for clarity\n",
    "                aggregated_df.columns = ['block_date', 'chain_id', \n",
    "                                        'total_len_encoded_transaction', 'average_len_encoded_transaction', 'transaction_count',\n",
    "                                        'total_estimatedSize', 'average_estimatedSize']\n",
    "                try:      \n",
    "                        dfs.append(aggregated_df)\n",
    "                except:\n",
    "                        print('nothing to append')\n",
    "                        continue\n",
    "\n",
    "aggregated_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aggregated_df['encoded_transaction'][0])\n",
    "# print(len(aggregated_df['encoded_transaction'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cols = ['average_len_encoded_transaction','average_estimatedSize','transaction_count']\n",
    "total_aggregated_df = aggregated_df[['chain_id'] + agg_cols].groupby(['chain_id']).mean([])\n",
    "total_aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Generate current timestamp\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Define the file path\n",
    "file_path = f\"outputs/l2_output_{current_timestamp}.csv\"\n",
    "# Save the DataFrame to CSV\n",
    "aggregated_df.to_csv(file_path, index=False)\n",
    "print(f\"DataFrame saved to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull aggregate L1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate L2 : L1 ratio metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
