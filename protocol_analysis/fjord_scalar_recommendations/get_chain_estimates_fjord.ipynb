{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas web3 hexbytes rlp fastlz clickhouse-connect\n",
    "# ! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme\n",
    "FastLZ needs Python version 3.9x or lower, make sure your environment is using a later python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from hexbytes import HexBytes\n",
    "import ast\n",
    "import rlp\n",
    "from rlp.sedes import Binary, big_endian_int, binary, List\n",
    "from eth_utils import to_bytes, to_hex\n",
    "import fastlz\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "sys.path.pop()\n",
    "\n",
    "client = ch.connect_to_clickhouse_db() #Default is OPLabs DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration\n",
    "chain_mappings_list = [\n",
    "        {'schema_name': 'op', 'display_name': 'OP Mainnet', 'chain_id': 10},\n",
    "        # {'schema_name': 'base', 'display_name': 'Base'},\n",
    "        # {'schema_name': 'mode', 'display_name': 'Mode'},\n",
    "        # {'schema_name': 'fraxtal', 'display_name': 'Fraxtal'},\n",
    "    # Add more mappings as needed\n",
    "]\n",
    "\n",
    "days_of_data = 7\n",
    "\n",
    "#FastLZ Regression Metrics\n",
    "# Specs - https://specs.optimism.io/fjord/exec-engine.html?search=#fjord-l1-cost-fee-changes-fastlz-estimator\n",
    "intercept = -42_585_600\n",
    "fastlzCoef = 836_500\n",
    "minTransactionSize = 100\n",
    "scaled_by = 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeDict({'blockHash': HexBytes('0x694fb0156a5719a8ee75ebf2568e8a8d55899db9312127f0d4fe5001c06ac397'), 'blockNumber': 120731426, 'chainId': 10, 'from': '0x899e837095a0F3CC62FB05998559Df90F26A1F46', 'gas': 69534, 'gasPrice': 4307539, 'hash': HexBytes('0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'), 'input': HexBytes('0xa9059cbb0000000000000000000000007a6e883eec3dd33528115637ea01b3b64e2f58490000000000000000000000000000000000000000000000009e34ef99a7740000'), 'nonce': 206, 'r': HexBytes('0x6727a53c0972c55923242cea052dc4e1105d7b65c91c442e2741440965eac357'), 's': HexBytes('0x0a8e71aea623adb7b5562fb9a779634f3b84dad7be1e1f22caaa640db352a6ff'), 'to': '0xdC6fF44d5d932Cbd77B52E5612Ba0529DC6226F1', 'transactionIndex': 4, 'type': 0, 'v': 55, 'value': 0})\n"
     ]
    }
   ],
   "source": [
    "# Test transaction receipt\n",
    "from web3 import Web3\n",
    "op_rpc = os.getenv(\"OP_PUBLIC_RPC\")\n",
    "w3 = Web3(Web3.HTTPProvider(op_rpc))\n",
    "\n",
    "tx_test = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "tx = w3.eth.get_transaction(tx_test)\n",
    "txr = w3.eth.get_transaction_receipt(tx_test)\n",
    "# # txraw = w3.eth.get_raw_transaction(tx_test)\n",
    "print(tx)\n",
    "# print(txr)\n",
    "# # print(txraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may not sufficent due to missing transaction signature fields\n",
    "\n",
    "# Get L2 Txs from Clickhouse / Goldsky\n",
    "query_by_day = '''\n",
    "        SELECT @chain_id@ as chain_id, nonce, gas, max_fee_per_gas, max_priority_fee_per_gas,\n",
    "                to_address as to, value, input, block_timestamp, block_number, hash, receipt_gas_used\n",
    "        FROM @chain_db_name@_transactions\n",
    "        WHERE gas_price > 0\n",
    "        # 1 day chunk\n",
    "        AND block_timestamp < DATE_TRUNC('day',NOW()) - interval '@day_num@ days'\n",
    "        AND block_timestamp >= DATE_TRUNC('day',NOW()) - (interval '@day_num@ days') - (interval '1 day')\n",
    "\n",
    "        SETTINGS max_execution_time = 3000\n",
    "'''\n",
    "# AND hash = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "# AND block_number = 120731426\n",
    "\n",
    "# txs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process transactions and RLP encode\n",
    "#https://ethereum.org/en/developers/docs/transactions/\n",
    "\n",
    "# NOTE THE RLP ENCODING IS NOT 1:1 WITH ETHERSCAN YET (but it's ~close-ish)\n",
    "def process_and_encode_transaction(row):\n",
    "    to_hexstr = 'eee'\n",
    "    try:\n",
    "        # Check if \"to\" field is None or empty\n",
    "        to_field = row[\"to\"]\n",
    "        if isinstance(to_field, bytes) and to_field == b'\\x00' * len(to_field):\n",
    "            to_hexstr = b''\n",
    "        else:\n",
    "            to_hexstr = to_bytes(hexstr=to_field.decode('utf-8')) if to_field is not None else b''\n",
    "        tx_params = {\n",
    "            'chainId': row['chain_id'],\n",
    "            'nonce': row['nonce'],\n",
    "            'maxPriorityFeePerGas': row['max_priority_fee_per_gas'],\n",
    "            'maxFeePerGas': row['max_fee_per_gas'],\n",
    "            'gas': row['gas'],\n",
    "            'to': to_hexstr,\n",
    "            'value': to_bytes(row['value']),\n",
    "            'input': HexBytes(row['input']),\n",
    "            'accessList': row['access_list'],  # This is assumed to be an empty list or properly formatted\n",
    "            'v': row['v'],\n",
    "            'r': HexBytes(row['r']),\n",
    "            's': HexBytes(row['s'])\n",
    "        }\n",
    "\n",
    "        transaction = [\n",
    "            # tx_params['chainId'],\n",
    "            # tx_params['nonce'],\n",
    "            # tx_params['gas'],\n",
    "            # tx_params['maxFeePerGas'],\n",
    "            # tx_params['maxPriorityFeePerGas'],\n",
    "            # tx_params['to'],\n",
    "            # tx_params['value'],\n",
    "            # tx_params['input'],\n",
    "            # tx_params['accessList'],\n",
    "            # tx_params['v'],\n",
    "            # tx_params['r'],\n",
    "            # tx_params['s']\n",
    "            # https://goethereumbook.org/en/transaction-raw-create/\n",
    "            tx_params['nonce'],\n",
    "            tx_params['to'],\n",
    "            tx_params['value'],\n",
    "            tx_params['gas'],\n",
    "            tx_params['maxFeePerGas'],\n",
    "            tx_params['input'],\n",
    "            tx_params['chainId'],\n",
    "            tx_params['v'],\n",
    "            tx_params['r'],\n",
    "            tx_params['s']\n",
    "        ]\n",
    "\n",
    "        encoded_tx = rlp.encode(transaction)\n",
    "        return Web3.to_hex(encoded_tx), len(encoded_tx)\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Failed Transaction Info:\")\n",
    "        print(row)\n",
    "        print(to_hexstr)\n",
    "        return None, None\n",
    "\n",
    "# Function to compress transaction data\n",
    "def compress_transaction(encoded_transaction):\n",
    "\n",
    "    hex_string = encoded_transaction[2:]\n",
    "    # Convert the hexadecimal string to bytes\n",
    "    byte_string = bytes.fromhex(hex_string)\n",
    "    compressed_data = fastlz.compress(byte_string)\n",
    "\n",
    "    return compressed_data.hex(), len(compressed_data)\n",
    "# Define a function to apply to each row of the DataFrame\n",
    "def process_and_compress_transaction(row):\n",
    "    encoded_tx = row['encoded_transaction']\n",
    "    compressed_tx, len_tx = compress_transaction(encoded_tx)\n",
    "    return compressed_tx, len_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op : day 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_64858/934000558.py:70: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  compressed_data = fastlz.compress(byte_string)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op : day 1\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for chain in chain_mappings_list:\n",
    "        for day_num in range(0,days_of_data+1):\n",
    "                print(chain['schema_name'] + ' : day ' + str(day_num))\n",
    "                query_map = query_by_day\n",
    "\n",
    "                query_map = query_map.replace(\"@chain_db_name@\", chain['schema_name'])\n",
    "                query_map = query_map.replace(\"@chain_id@\", str(chain['chain_id']))\n",
    "                query_map = query_map.replace(\"@day_num@\", str(day_num))\n",
    "\n",
    "                result_df = client.query_df(query_map)\n",
    "                print ('        query done')\n",
    "                # Add Dummy Signature and fields\n",
    "                result_df['access_list'] = '[]'\n",
    "                result_df['access_list'] = result_df['access_list'].apply(ast.literal_eval)\n",
    "                result_df['r'] = '0x6727a53c0972c55923242cea052dc4e1105d7b65c91c442e2741440965eac357'\n",
    "                result_df['s'] = '0x0a8e71aea623adb7b5562fb9a779634f3b84dad7be1e1f22caaa640db352a6ff'\n",
    "                result_df['v'] = '55'\n",
    "\n",
    "                # Assuming `txs_df` is your DataFrame\n",
    "                result_df[['encoded_transaction', 'len_encoded_transaction']] = result_df.apply(process_and_encode_transaction, axis=1, result_type='expand')\n",
    "                print ('        encoding done')\n",
    "                # Apply compression to each transaction in the DataFrame\n",
    "                result_df[['compressed_transaction', 'compressed_transaction_length']] = result_df.apply(process_and_compress_transaction, axis=1, result_type='expand')\n",
    "                print ('        compression done')\n",
    "                # Calculate estimated size for each row\n",
    "                result_df['estimatedSize_raw'] = result_df.apply(lambda row: (intercept + (row['compressed_transaction_length'] * fastlzCoef)) / scaled_by, axis=1)\n",
    "                # Calculate minimum value for 'estimatedSize' column\n",
    "                result_df['estimatedSize'] = result_df.apply(lambda row: max(minTransactionSize, row['estimatedSize_raw']), axis=1)\n",
    "                print ('        estimation done')\n",
    "\n",
    "                # Agg L2\n",
    "                # Convert block_timestamp to date (truncate to day)\n",
    "                result_df['block_date'] = pd.to_datetime(result_df['block_timestamp']).dt.date\n",
    "                grouped_df = result_df.groupby(['block_date', 'chain_id'])\n",
    "                # Define aggregation functions\n",
    "                agg_functions = {\n",
    "                        'len_encoded_transaction': ['sum', 'mean', 'count'],\n",
    "                        'estimatedSize': ['sum', 'mean']\n",
    "                }\n",
    "                # Perform aggregation\n",
    "                aggregated_df = grouped_df.agg(agg_functions).reset_index()\n",
    "                # Rename columns for clarity\n",
    "                aggregated_df.columns = ['block_date', 'chain_id', \n",
    "                                        'total_len_encoded_transaction', 'average_len_encoded_transaction', 'transaction_count',\n",
    "                                        'total_estimatedSize', 'average_estimatedSize']\n",
    "                        \n",
    "                dfs.append(aggregated_df)\n",
    "                dfs.append(result_df)\n",
    "\n",
    "aggregated_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aggregated_df['encoded_transaction'][0])\n",
    "# print(len(aggregated_df['encoded_transaction'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cols = ['average_len_encoded_transaction','average_estimatedSize','transaction_count']\n",
    "total_aggregated_df = aggregated_df[['chain_id'] + agg_cols].groupby(['chain_id']).mean([])\n",
    "total_aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Generate current timestamp\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Define the file path\n",
    "file_path = f\"outputs/l2_output_{current_timestamp}.csv\"\n",
    "# Save the DataFrame to CSV\n",
    "aggregated_df.to_csv(file_path, index=False)\n",
    "print(f\"DataFrame saved to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull aggregate L1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate L2 : L1 ratio metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
