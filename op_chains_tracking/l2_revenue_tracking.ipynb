{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade dune-client\n",
    "# ! pip show dune-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get L2 Revenue and post it to a database (csv in github for now)\n",
    "# TODO - Integrate with BQ upload\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "sys.path.append(\"../helper_functions\")\n",
    "# import web3py_utils as w3py\n",
    "import duneapi_utils as du\n",
    "import google_bq_utils as bqu\n",
    "from web3 import Web3\n",
    "from datetime import datetime, timezone\n",
    "sys.path.pop()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ethereum-optimism/optimism/blob/b86522036ad11a91de1d1dadb6805167add83326/specs/predeploys.md?plain=1#L50\n",
    "\n",
    "# Name, contract\n",
    "fee_vaults = [\n",
    "    ['SequencerFeeVault','0x4200000000000000000000000000000000000011'],\n",
    "    ['BaseFeeVault','0x4200000000000000000000000000000000000019'],\n",
    "    ['L1FeeVault','0x420000000000000000000000000000000000001A'],\n",
    "]\n",
    "\n",
    "# Aiming to eventually read from superchain-resitry + some non-superchain static adds\n",
    "chains_rpcs = pd.read_csv('outputs/chain_metadata.csv', na_filter=False)\n",
    "# print(chains_rpcs.columns)\n",
    "chains_rpcs = chains_rpcs[~(chains_rpcs['rpc_url'] == '') & ~(chains_rpcs['op_based_version'].str.contains('legacy'))]\n",
    "# chains_rpcs = chains_rpcs.values.tolist()\n",
    "# print(chains_rpcs.sample(5))\n",
    "\n",
    "# # Temp\n",
    "# chains_rpcs = chains_rpcs.head(1)\n",
    "# chains_rpcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method ID: 0x84411d65\n"
     ]
    }
   ],
   "source": [
    "# Calculate the method signature hash\n",
    "method_signature = \"totalProcessed()\"\n",
    "method_id = Web3.keccak(text=method_signature)[:4].hex()\n",
    "# Verify the method ID\n",
    "print(f\"Method ID: {method_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [\n",
    "    'block_time', 'block_number', 'chain_name', 'vault_name', \n",
    "    'vault_address', 'alltime_revenue_native', 'chain_id'\n",
    "    # 'gas_token', 'da_layer', 'is_superchain_registry' # Uncomment these if needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr = []\n",
    "\n",
    "for index, chain in chains_rpcs.iterrows():\n",
    "    chain_name = chain['chain_name']\n",
    "    chain_id = chain['mainnet_chain_id']\n",
    "\n",
    "    # if chain_id:  # Check if chain_id is not empty\n",
    "    #     chain_id = int(float(chain_id))  # Convert to float first, then to int\n",
    "    # else:\n",
    "    #     chain_id = None  # or keep it as an empty string or any default value you prefer\n",
    "        \n",
    "    print(chain_name + ' - ' + str(chain_id))\n",
    "    rpc = chain['rpc_url']\n",
    "    # gas_token = chain['gas_token']\n",
    "    # da_layer = chain['da_layer']\n",
    "    # is_superchain_registry = chain['superchain_registry']\n",
    "\n",
    "    try:\n",
    "        w3_conn = Web3(Web3.HTTPProvider(rpc))\n",
    "        # Get the timestamp of the latest block\n",
    "        block_timestamp = w3_conn.eth.get_block('latest').timestamp\n",
    "        block_number = w3_conn.eth.get_block('latest').number\n",
    "        # Convert the UNIX timestamp to a human-readable format\n",
    "        block_datetime = datetime.fromtimestamp(block_timestamp, tz=timezone.utc)\n",
    "        block_time = block_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        for vault in fee_vaults:\n",
    "            vault_name = vault[0]\n",
    "            vault_address = vault[1]\n",
    "            # Call the function directly using eth_call\n",
    "            response = w3_conn.eth.call({\n",
    "                'to': vault_address,\n",
    "                'data': method_id\n",
    "            })\n",
    "            wei_balance = w3_conn.eth.get_balance(vault_address)\n",
    "            # Decode the result (assuming the function returns a uint256)\n",
    "            proxy_processed_wei = Web3.to_int(hexstr=response.hex())\n",
    "            \n",
    "            alltime_revenue_wei = proxy_processed_wei+wei_balance\n",
    "            alltime_revenue_native = alltime_revenue_wei/1e18\n",
    "\n",
    "            print(chain_name + ' | ' + vault_name + ': ' \\\n",
    "                + str(proxy_processed_wei) + ' | bal: ' + str(wei_balance)\\\n",
    "                + ' | total eth: ' + str( (alltime_revenue_native) )\n",
    "                )\n",
    "            \n",
    "            tmp = pd.DataFrame(\n",
    "                    [[block_time, block_number, chain_name, vault_name, vault_address, alltime_revenue_native, chain_id]]#, gas_token, da_layer, is_superchain_registry]]\n",
    "                    ,columns =df_columns\n",
    "                    )\n",
    "            data_arr.append(tmp)\n",
    "            time.sleep(1)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "data_df = pd.concat(data_arr)\n",
    "data_df['block_time'] = pd.to_datetime(data_df['block_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/all_time_revenue_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdata_df\u001b[49m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = 'outputs/all_time_revenue_data.csv'\n",
    "\n",
    "data_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Ensure the existing file has the same columns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m existing_df \u001b[38;5;241m=\u001b[39m existing_df\u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mdf_columns)\n\u001b[0;32m----> 7\u001b[0m data_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m\u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mdf_columns)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Append without writing the header\u001b[39;00m\n\u001b[1;32m      9\u001b[0m data_df\u001b[38;5;241m.\u001b[39mto_csv(file_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Read the existing CSV file\n",
    "    existing_df = pd.read_csv(file_path)\n",
    "    # Ensure the existing file has the same columns\n",
    "    existing_df = existing_df.reindex(columns=df_columns)\n",
    "    data_df = data_df.reindex(columns=df_columns)\n",
    "    # Append without writing the header\n",
    "    data_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # If file doesn't exist, create it and write the header\n",
    "    data_df.to_csv(file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Overwrite to Dune Table\n",
    "dune_df = pd.read_csv(file_path)\n",
    "# print(dune_df.sample(5))\n",
    "# du.write_dune_api_from_pandas(dune_df, 'op_stack_chains_cumulative_revenue_snapshots',\\\n",
    "#                              'Snapshots of All-Time (cumulative) revenue for fee vaults on OP Stack Chains. Pulled from RPCs - metadata in op_stack_chains_chain_rpc_metdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert Updates to Dune Table\n",
    "create_namespace = 'oplabspbc'\n",
    "create_table_name = 'op_stack_chains_cumulative_revenue_snapshots'\n",
    "create_table_description = 'Snapshots of All-Time (cumulative) revenue for fee vaults on OP Stack Chains. Pulled from RPCs - metadata in op_stack_chains_chain_rpc_metdata'\n",
    "# try:\n",
    "du.create_dune_table(data_df, namespace = create_namespace\n",
    "                        , table_name = create_table_name\n",
    "                        , table_description = create_table_description)\n",
    "\n",
    "# except:\n",
    "        # print('error creating')\n",
    "du.insert_dune_api_from_pandas(data_df, namespace = create_namespace,table_name = create_table_name)\n",
    "\n",
    "du.write_dune_api_from_pandas(chains_rpcs, 'op_stack_chains_chain_rpc_metdata',\\\n",
    "                             'Chain metadata - used to join with op_stack_chains_cumulative_revenue_snapshots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_cols = ['block_time','block_number','chain_name','vault_name','vault_address','alltime_revenue_native','chain_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dune_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "block_time                datetime64[ns]\n",
       "block_number                       int64\n",
       "chain_name                        object\n",
       "vault_name                        object\n",
       "vault_address                     object\n",
       "alltime_revenue_native           float64\n",
       "chain_id                         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dune_df['block_time'] = pd.to_datetime(dune_df['block_time'])\n",
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "Data loaded successfully to rpc_table_uploads.hourly_cumulative_l2_revenue_snapshots\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'rpc_table_uploads'\n",
    "table_name = 'hourly_cumulative_l2_revenue_snapshots'\n",
    "\n",
    "# Write All to BQ Table\n",
    "# bqu.write_df_to_bq_table(df = dune_df[bq_cols], table_id = table_name, dataset_id = dataset_name)\n",
    "\n",
    "# Write Updates to BQ Table\n",
    "unique_cols = ['block_time', 'chain_name', 'chain_id', 'vault_name']\n",
    "bqu.write_df_to_bq_table(df = data_df[bq_cols], table_id = table_name, dataset_id = dataset_name, write_mode='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
