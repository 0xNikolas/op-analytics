{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See readme.md for ideal fields and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can do installs here when conde env is activated\n",
    "# ! pip install pandas pyarrow\n",
    "# ! pip install polars\n",
    "# ! pip install maturin\n",
    "# ! pip install cryo\n",
    "# ! pip install web3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import cryo\n",
    "import sys\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import web3py_utils as w3py\n",
    "import os_utils as osu\n",
    "sys.path.pop()\n",
    "import polars as pl\n",
    "# test adding cryo_cli python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'cryo_outputs/'\n",
    "# clear out\n",
    "osu.clear_folder(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "redstone = {\n",
    "        'rpc_url': 'https://rpc.redstonechain.com',\n",
    "        'chain_name': 'redstone',\n",
    "        'block_time_sec': 2,\n",
    "        'block_time_buffer': 0,\n",
    "        'stack': 'op'\n",
    "        }\n",
    "# Lyra\n",
    "lyra = {\n",
    "        'rpc_url': 'https://rpc.lyra.finance/',\n",
    "        'chain_name': 'lyra',\n",
    "        'block_time_sec': 2,\n",
    "        'block_time_buffer': 0,\n",
    "        'stack': 'op'\n",
    "        }\n",
    "degen = {\n",
    "        'rpc_url': 'https://rpc.degen.tips',\n",
    "        'chain_name': 'degen',\n",
    "        'block_time_sec': 0.383, #Note: Arb Stack not deterministic\n",
    "        'block_time_buffer': 0.25,\n",
    "        'stack': 'arb'\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick Chain\n",
    "config_chain = degen\n",
    "datasets = ['txs', 'logs', 'blocks']\n",
    "###\n",
    "whole_day_only = True\n",
    "trailing_days = 0.5\n",
    "dry_run = False\n",
    "requests_per_second_max = 500 # -1 means ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediate Calc\n",
    "rpc_url = config_chain['rpc_url']\n",
    "stack = config_chain['stack']\n",
    "chain_name = config_chain['chain_name']\n",
    "# block_time_sec = config_chain['block_time_sec']\n",
    "# block_time_buffer = config_chain['block_time_buffer']\n",
    "\n",
    "# blocks_per_day = (60*60*24) / block_time_sec\n",
    "# blocks_per_day_lo = (60*60*24) / (block_time_sec+block_time_buffer)\n",
    "# if block_time_sec-block_time_buffer > 0:\n",
    "#         blocks_per_day_hi = (60*60*24) / (block_time_sec-block_time_buffer)\n",
    "# else:\n",
    "#         blocks_per_day_hi = (60*60*24) / 0.01\n",
    "\n",
    "# print(blocks_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init timestamps\n",
    "\n",
    "# Get the current time in UTC\n",
    "current_time_utc = datetime.utcnow()\n",
    "# print(current_time_utc)\n",
    "\n",
    "# If only whole days then shift the ending time to the start of the day\n",
    "if whole_day_only:\n",
    "        current_date_utc = datetime.combine(datetime.utcnow().date(), datetime.min.time())\n",
    "        time_difference = current_time_utc - current_date_utc\n",
    "        difference_days_fraction = time_difference.total_seconds() / (24 * 3600)  # There are 86400 seconds in a day\n",
    "else: \n",
    "        current_date_utc = current_time_utc\n",
    "        difference_days_fraction = 0\n",
    "print('day fraction :' + str(difference_days_fraction))\n",
    "\n",
    "print(current_date_utc)\n",
    "starting_date_utc = current_date_utc - timedelta(days=trailing_days)\n",
    "\n",
    "current_block = w3py.getLatestBlockNumber(rpc_url)\n",
    "\n",
    "# ending_block = int( current_block - (difference_days_fraction * blocks_per_day_lo) )\n",
    "# starting_block = int( ending_block - (trailing_days * blocks_per_day_hi) )\n",
    "\n",
    "# print('current: ' + str(int(current_block)))\n",
    "# print('end: ' + str(int(ending_block)))\n",
    "# print('start: ' + str(int(starting_block)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = int(starting_date_utc.timestamp())\n",
    "end_timestamp = int(current_date_utc.timestamp())\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and save data in JSON\n",
    "data = cryo.freeze(\n",
    "    datasets,\n",
    "    # blocks=[str(starting_block) + \":\" + str(ending_block)],\n",
    "    timestamps=[str(start_timestamp) + \":\" + str(end_timestamp)],\n",
    "    rpc=rpc_url,\n",
    "    output_dir= output_directory,\n",
    "    file_format='parquet',\n",
    "    label=chain_name,\n",
    "    hex=True,\n",
    "    dry=dry_run,\n",
    "    requests_per_second=requests_per_second_max\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "# Print the elapsed time in seconds\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move this to a cryo_utils eventually\n",
    "\n",
    "def load_parquet_files(chain_name, data_type_name, data_directory):\n",
    "    pattern = f\"*{data_type_name}__{chain_name}*.parquet\"\n",
    "    file_paths = glob.glob(os.path.join(data_directory, pattern))\n",
    "    \n",
    "    if file_paths:\n",
    "        df = pl.scan_parquet(file_paths)\n",
    "        # Further processing can go here, for example:\n",
    "        # df = df.filter(pl.col(\"some_column\") > 0)\n",
    "        return df.collect()  # Collecting after all transformations\n",
    "    else:\n",
    "        print(\"No files found matching the pattern.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read parquet files\n",
    "blocks = load_parquet_files(chain_name, 'blocks', output_directory)\n",
    "txs = load_parquet_files(chain_name, 'transactions', output_directory)\n",
    "logs = load_parquet_files(chain_name, 'logs', output_directory)\n",
    "\n",
    "# Rename the 'gas_used' column to 'block_gas_used' in the 'blocks' DataFrame\n",
    "blocks = blocks.rename({\"gas_used\": \"block_gas_used\"})\n",
    "print(txs.columns)\n",
    "print(blocks.columns)\n",
    "print(logs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the exclude_topics from CSV\n",
    "non_app_methods_df = pl.read_csv(\"../inputs/non_app_methods.csv\")\n",
    "# print(non_app_methods_df)\n",
    "exclude_topics = non_app_methods_df.filter(non_app_methods_df['type'] == \"topic0\")[\"method_id\"].to_list()\n",
    "# print(exclude_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by block_number, transaction_hash, and chain_id and count rows\n",
    "logs_agg = (\n",
    "    logs.group_by([pl.col(\"block_number\"), pl.col(\"transaction_hash\"), pl.col(\"chain_id\")])\n",
    "    .agg(\n",
    "        num_event_logs = pl.col(\"topic0\").count(),\n",
    "        num_app_event_logs = pl.col(\"topic0\").filter(~pl.col(\"topic0\").is_in(exclude_topics)).count()\n",
    "    )\n",
    ")\n",
    "# logs_agg[['transaction_hash','num_event_logs','num_app_event_logs']].glimpse()\n",
    "# logs_agg.filter(pl.col(\"num_event_logs\") != pl.col(\"num_app_event_logs\")).glimpse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform the join on 'block_number' and 'chain_id'\n",
    "joined_df = blocks.join(\n",
    "    txs,\n",
    "    on=[\"block_number\", \"chain_id\"],\n",
    "    how=\"inner\"  # You can specify the type of join you want (inner, outer, left, right)\n",
    ")\n",
    "joined_df = joined_df.join(\n",
    "    logs_agg,\n",
    "    on=[\"block_number\", \"chain_id\", \"transaction_hash\"],\n",
    "    how=\"left\"  # You can specify the type of join you want (inner, outer, left, right)\n",
    ")\n",
    "# Convert Unix timestamp to datetime and create a new column 'timestamp_dt'\n",
    "joined_df = joined_df.with_columns(\n",
    "    pl.from_epoch(\"timestamp\", time_unit=\"s\").alias(\"timestamp_dt\")\n",
    ")\n",
    "\n",
    "# Truncate the 'timestamp_dt' column to the day and create a new column 'timestamp_date'\n",
    "joined_df = joined_df.with_columns(\n",
    "    pl.col(\"timestamp_dt\").dt.truncate(\"1d\").alias(\"timestamp_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(blocks.schema)\n",
    "# print(txs.schema)\n",
    "print(joined_df.schema)\n",
    "print(type(joined_df))\n",
    "\n",
    "#test output\n",
    "joined_pd = joined_df.to_pandas()\n",
    "# print(joined_pd.tail(5))\n",
    "\n",
    "print('num blocks: ' + str(joined_pd['block_number'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'joined_df' with the required columns\n",
    "\n",
    "if stack == 'op':\n",
    "        l2_gas_used = pl.col(\"gas_used\")\n",
    "        l1_gas_used = pl.col(\"l1_gas_used\")\n",
    "elif stack == 'arb':\n",
    "        # Need to add columns to cryo\n",
    "        l2_gas_used = pl.col(\"gas_used\") - pl.col(\"gas_used_for_l1\")\n",
    "        l1_gas_used = pl.col(\"gas_used_for_l1\")\n",
    "else:\n",
    "        l2_gas_used = pl.col(\"gas_used\")\n",
    "        l1_gas_used = pl.col(\"gas_used\") - pl.col(\"gas_used\")\n",
    "\n",
    "result_df = joined_df \\\n",
    "        .filter(pl.col(\"timestamp\").between(start_timestamp, end_timestamp)) \\\n",
    "        .group_by([pl.col(\"timestamp_date\"), pl.col(\"chain_id\")]).agg(\n",
    "                num_blocks=pl.col(\"block_number\").n_unique(),\n",
    "                min_block_number=pl.col(\"block_number\").min(),\n",
    "                max_block_number=pl.col(\"block_number\").max(),\n",
    "                min_block_time=pl.col(\"timestamp\").min(),\n",
    "                max_block_time=pl.col(\"timestamp\").max(),\n",
    "\n",
    "                num_user_transactions=\n",
    "                pl.when(pl.col(\"gas_price\") > 0).then(pl.col(\"transaction_hash\")).count(),\n",
    "                num_success_user_transactions=\n",
    "                pl.when((pl.col(\"gas_price\") > 0) & pl.col(\"success\")).then(pl.col(\"transaction_hash\")).count(),\n",
    "                num_success_event_transactions=\n",
    "                pl.when((pl.col(\"gas_price\") > 0) & pl.col(\"success\") & (pl.col(\"num_event_logs\") > 0)).then(pl.col(\"transaction_hash\")).count(),\n",
    "                num_success_qualified_transactions=\n",
    "                pl.when((pl.col(\"gas_price\") > 0) & pl.col(\"success\") & (pl.col(\"num_app_event_logs\") > 0)).then(pl.col(\"transaction_hash\")).count(),\n",
    "                num_senders=pl.col(\"from_address\").filter(pl.col(\"gas_price\") > 0).n_unique(),\n",
    "\n",
    "                total_gas_used=pl.col(\"gas_used\").sum(),\n",
    "                user_gas_used=pl.col(\"gas_used\").filter(pl.col(\"gas_price\") > 0).sum(),\n",
    "                total_gas_used_per_block = pl.col(\"gas_used\").sum() / pl.col(\"block_number\").n_unique(),\n",
    "                user_gas_used_per_block = pl.col(\"gas_used\").filter(pl.col(\"gas_price\") > 0).sum() / pl.col(\"block_number\").n_unique(),\n",
    "\n",
    "                # l2_fees_base_fees_eth=( (pl.col(\"gas_price\") - pl.col(\"max_priority_fee_per_gas\")) * pl.col(\"gas_used\")).sum() / 1e18,\n",
    "                # l2_fees_priority_fees_eth=pl.when(pl.col(\"gas_price\") > 0).then(pl.col(\"max_priority_fee_per_gas\") * pl.col(\"gas_used\")).sum() / 1e18,\n",
    "                # l2_fees_total_fees_eth=(pl.col(\"gas_price\") * pl.col(\"gas_used\")).sum() / 1e18,\n",
    "\n",
    "                avg_l2_base_fee_gwei=( (pl.col(\"gas_price\") - pl.col(\"max_priority_fee_per_gas\")) * pl.col(\"gas_used\")).sum()\n",
    "                                        / pl.col(\"gas_used\").sum() /1e9,\n",
    "                avg_l2_priority_fee_gwei=pl.when(pl.col(\"gas_price\") > 0).then((pl.col(\"max_priority_fee_per_gas\")) * pl.col(\"gas_used\")).sum()\n",
    "                                        / pl.col(\"gas_used\").sum() /1e9,\n",
    "        )\n",
    "# timestamp formats\n",
    "result_df = result_df.with_columns([\n",
    "    pl.date_from_epoch_seconds(pl.col(\"min_block_time\")).alias(\"min_block_time_dt\"),\n",
    "    pl.date_from_epoch_seconds(pl.col(\"max_block_time\")).alias(\"max_block_time_dt\")\n",
    "])\n",
    "\n",
    "result_df = result_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.write_csv(f\"outputs/{chain_name}_{start_timestamp}_{end_timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "# result_df = result_df[ (result_df['min_block_time']>= start_timestamp ) & (result_df['min_block_time']<= end_timestamp ) ]\n",
    "#seems like 1 block before gets pulled. yolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['min_block_time_dt'] = pd.to_datetime(result_df['min_block_time'], unit='s')\n",
    "result_df['max_block_time_dt'] = pd.to_datetime(result_df['max_block_time'], unit='s')\n",
    "display(result_df.sort_values(by='timestamp_date',ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
