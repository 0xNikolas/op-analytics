{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See readme.md for ideal fields and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can do installs here when conde env is activated\n",
    "# ! pip install pandas pyarrow\n",
    "# ! pip install polars\n",
    "# ! pip install maturin\n",
    "# ! pip install cryo\n",
    "# ! pip install web3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import cryo\n",
    "import sys\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import web3py_utils as w3py\n",
    "import os_utils as osu\n",
    "sys.path.pop()\n",
    "import polars as pl\n",
    "# test adding cryo_cli python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'cryo_outputs/'\n",
    "# clear out\n",
    "osu.clear_folder(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test doing Degen Chain\n",
    "config_chain = {\n",
    "        'rpc_url': 'https://rpc.redstonechain.com',\n",
    "        'chain_name': 'redstone',\n",
    "        'block_time_sec': 2,\n",
    "        'block_time_buffer': 0\n",
    "        }\n",
    "# Lyra\n",
    "# 'rpc_url': 'https://rpc.lyra.finance/'\n",
    "# 'chain_name': 'lyra'\n",
    "# 'block_time_sec': 2\n",
    "# 'block_time_buffer': 0\n",
    "# DEGEN\n",
    "# 'rpc_url': 'https://rpc.degen.tips',\n",
    "# 'chain_name': 'degen',\n",
    "# 'block_time_sec': 0.383, #Note: Arb Stack not deterministic\n",
    "# 'block_time_buffer': 0.25\n",
    "\n",
    "###\n",
    "whole_day_only = False#True\n",
    "trailing_days = 0.25\n",
    "dry_run = False\n",
    "fields = ['blocks', 'txs']\n",
    "requests_per_second_max = 500 # -1 means ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediate Calc\n",
    "rpc_url = config_chain['rpc_url']\n",
    "chain_name = config_chain['chain_name']\n",
    "block_time_sec = config_chain['block_time_sec']\n",
    "block_time_buffer = config_chain['block_time_buffer']\n",
    "\n",
    "blocks_per_day = (60*60*24) / block_time_sec\n",
    "blocks_per_day_lo = (60*60*24) / (block_time_sec+block_time_buffer)\n",
    "if block_time_sec-block_time_buffer > 0:\n",
    "        blocks_per_day_hi = (60*60*24) / (block_time_sec-block_time_buffer)\n",
    "else:\n",
    "        blocks_per_day_hi = (60*60*24) / 0.01\n",
    "\n",
    "print(blocks_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init timestamps\n",
    "\n",
    "# Get the current time in UTC\n",
    "current_time_utc = datetime.utcnow()\n",
    "print(current_time_utc)\n",
    "\n",
    "# If only whole days then shift the ending time to the start of the day\n",
    "if whole_day_only:\n",
    "        current_date_utc = datetime.combine(datetime.utcnow().date(), datetime.min.time())\n",
    "        time_difference = current_time_utc - current_date_utc\n",
    "        difference_days_fraction = time_difference.total_seconds() / (24 * 3600)  # There are 86400 seconds in a day\n",
    "else: \n",
    "        current_date_utc = current_time_utc\n",
    "        difference_days_fraction = 0\n",
    "print('day fraction :' + str(difference_days_fraction))\n",
    "\n",
    "print(current_date_utc)\n",
    "starting_date_utc = current_date_utc - timedelta(days=trailing_days)\n",
    "\n",
    "current_block = w3py.getLatestBlockNumber(rpc_url)\n",
    "\n",
    "ending_block = int( current_block - (difference_days_fraction * blocks_per_day_lo) )\n",
    "starting_block = int( ending_block - (trailing_days * blocks_per_day_hi) )\n",
    "\n",
    "print('current: ' + str(int(current_block)))\n",
    "print('end: ' + str(int(ending_block)))\n",
    "print('start: ' + str(int(starting_block)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timestamp = int(starting_date_utc.timestamp())\n",
    "end_timestamp = int(current_date_utc.timestamp())\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Command\n",
    "# if dry_run == 1:\n",
    "#     dry_txt = '--dry'\n",
    "# else:\n",
    "#     dry_txt = ''\n",
    "# if requests_per_second_max > -1:\n",
    "#     rps_txt = '--requests-per-second ' + str(requests_per_second_max)\n",
    "# else:\n",
    "#     rps_txt = ''\n",
    "\n",
    "# Fetch and save blocks data in JSON\n",
    "data = cryo.freeze(\n",
    "    ['txs', 'blocks'],\n",
    "    blocks=[str(starting_block) + \":\" + str(ending_block)],\n",
    "    rpc=rpc_url,\n",
    "    output_dir= output_directory,\n",
    "    file_format='parquet',\n",
    "    label=chain_name,\n",
    "    hex=True,\n",
    "    dry=dry_run,\n",
    "    requests_per_second=requests_per_second_max\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "# Print the elapsed time in seconds\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move this to a cryo_utils eventually\n",
    "\n",
    "def load_parquet_files(chain_name, data_type_name, data_directory):\n",
    "    pattern = f\"*{data_type_name}__{chain_name}*.parquet\"\n",
    "    file_paths = glob.glob(os.path.join(data_directory, pattern))\n",
    "    \n",
    "    if file_paths:\n",
    "        df = pl.scan_parquet(file_paths)\n",
    "        # Further processing can go here, for example:\n",
    "        # df = df.filter(pl.col(\"some_column\") > 0)\n",
    "        return df.collect()  # Collecting after all transformations\n",
    "    else:\n",
    "        print(\"No files found matching the pattern.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read parquet files\n",
    "txs = load_parquet_files(chain_name, 'transactions', output_directory)\n",
    "blocks = load_parquet_files(chain_name, 'blocks', output_directory)\n",
    "\n",
    "# Rename the 'gas_used' column to 'block_gas_used' in the 'blocks' DataFrame\n",
    "blocks = blocks.rename({\"gas_used\": \"block_gas_used\"})\n",
    "\n",
    "# Perform the join on 'block_number' and 'chain_id'\n",
    "joined_df = blocks.join(\n",
    "    txs,\n",
    "    on=[\"block_number\", \"chain_id\"],\n",
    "    how=\"inner\"  # You can specify the type of join you want (inner, outer, left, right)\n",
    ")\n",
    "\n",
    "# Convert Unix timestamp to datetime and create a new column 'timestamp_dt'\n",
    "joined_df = joined_df.with_columns(\n",
    "    pl.from_epoch(\"timestamp\", time_unit=\"s\").alias(\"timestamp_dt\")\n",
    ")\n",
    "\n",
    "# Truncate the 'timestamp_dt' column to the day and create a new column 'timestamp_date'\n",
    "joined_df = joined_df.with_columns(\n",
    "    pl.col(\"timestamp_dt\").dt.truncate(\"1d\").alias(\"timestamp_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(blocks.schema)\n",
    "# print(txs.schema)\n",
    "print(joined_df.schema)\n",
    "print(type(joined_df))\n",
    "\n",
    "#test output\n",
    "joined_pd = joined_df.to_pandas()\n",
    "# print(joined_pd.tail(5))\n",
    "\n",
    "print('num blocks: ' + str(joined_pd['block_number'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'joined_df' with the required columns\n",
    "\n",
    "result_df = joined_df.group_by([pl.col(\"timestamp_date\"), pl.col(\"chain_id\")]).agg(\n",
    "    num_blocks=pl.col(\"block_number\").n_unique(),\n",
    "    min_block_number=pl.col(\"block_number\").min(),\n",
    "    max_block_number=pl.col(\"block_number\").max(),\n",
    "    min_block_time=pl.col(\"timestamp\").min(),\n",
    "    max_block_time=pl.col(\"timestamp\").max(),\n",
    "\n",
    "    num_user_transactions=\n",
    "        pl.when(pl.col(\"gas_price\") > 0).then(pl.col(\"transaction_hash\")).count(),\n",
    "    num_success_user_transactions=\n",
    "        pl.when((pl.col(\"gas_price\") > 0) & pl.col(\"success\")).then(pl.col(\"transaction_hash\")).count(),\n",
    "    num_senders=pl.col(\"from_address\").filter(pl.col(\"gas_price\") > 0).n_unique(),\n",
    "\n",
    "    total_gas_used=pl.col(\"gas_used\").sum(),\n",
    "    user_gas_used=pl.col(\"gas_used\").filter(pl.col(\"gas_price\") > 0).sum(),\n",
    "    total_gas_used_per_block = pl.col(\"gas_used\").sum() / pl.col(\"block_number\").n_unique(),\n",
    "    user_gas_used_per_block = pl.col(\"gas_used\").filter(pl.col(\"gas_price\") > 0).sum() / pl.col(\"block_number\").n_unique(),\n",
    "    \n",
    "    l2_fees_base_fees_eth=(pl.col(\"base_fee_per_gas\") * pl.col(\"gas_used\")).sum() / 1e18,\n",
    "    l2_fees_priority_fees_eth=pl.when(pl.col(\"gas_price\") > 0).then((pl.col(\"gas_price\") - pl.col(\"base_fee_per_gas\")) * pl.col(\"gas_used\")).sum() / 1e18,\n",
    "    l2_fees_total_fees_eth=(pl.col(\"gas_price\") * pl.col(\"gas_used\")).sum() / 1e18,\n",
    ")\n",
    "result_df = result_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "result_df = result_df[ (result_df['min_block_time']>= start_timestamp ) & (result_df['min_block_time']<= end_timestamp ) ]\n",
    "#seems like 1 block before gets pulled. yolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['min_block_time_dt'] = pd.to_datetime(result_df['min_block_time'], unit='s')\n",
    "result_df['max_block_time_dt'] = pd.to_datetime(result_df['max_block_time'], unit='s')\n",
    "display(result_df.sort_values(by='timestamp_date',ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
