{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start l2 activity\n"
     ]
    }
   ],
   "source": [
    "print('start l2 activity')\n",
    "import sys\n",
    "sys.path.append(\"../helper_functions\")\n",
    "import duneapi_utils as d\n",
    "import growthepieapi_utils as gtp\n",
    "import l2beat_utils as ltwo\n",
    "import csv_utils as cu\n",
    "import google_bq_utils as bqu\n",
    "import pandas_utils as pu\n",
    "sys.path.pop()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Usage\n",
    "gtp_api = gtp.get_growthepie_api_data()\n",
    "gtp_meta_api = gtp.get_growthepie_api_meta()\n",
    "gtp_api = gtp_api.rename(columns={'date':'dt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2beat_aoc = ltwo.get_daily_aoc_by_token()\n",
    "l2beat_aoc = l2beat_aoc.rename(columns={'project':'chain','date':'dt'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2beat_df = ltwo.get_all_l2beat_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_63216/2114119210.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  l2beat_meta['is_upcoming'] = l2beat_meta['is_upcoming'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "l2beat_meta = ltwo.get_l2beat_metadata()\n",
    "l2beat_meta['chain'] = l2beat_meta['slug']\n",
    "l2beat_meta['is_upcoming'] = l2beat_meta['is_upcoming'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_l2b_df = l2beat_df.merge(l2beat_meta[\n",
    "        ['chain','name','layer','chainId','provider','provider_entity','category',\\\n",
    "         'is_upcoming','is_archived','is_current_chain']\n",
    "        ], on='chain',how='outer')\n",
    "\n",
    "combined_l2b_df['chainId'] = combined_l2b_df['chainId'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gtp_df = gtp_api.merge(gtp_meta_api[['origin_key','chain_name']], on='origin_key',how='left')\n",
    "combined_gtp_df[\"dt\"] = pd.to_datetime(combined_gtp_df[\"dt\"], errors='coerce')\n",
    "\n",
    "combined_gtp_df = combined_gtp_df.drop(columns=('index'))\n",
    "# combined_gtp_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Columns\n",
    "# Assuming combined_gtp_df is your DataFrame\n",
    "column_names = combined_gtp_df.columns\n",
    "\n",
    "for col in column_names:\n",
    "    if col.endswith('_usd'):\n",
    "        # Construct the new column name by replacing '_usd' with '_eth'\n",
    "        new_col_name = col.replace('_usd', '_eth')\n",
    "        \n",
    "        # Check if the new column name exists in the DataFrame\n",
    "        if new_col_name not in combined_gtp_df.columns:\n",
    "            # If it doesn't exist, create the column and fill it with nan values\n",
    "            combined_gtp_df[new_col_name] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Metadata\n",
    "opstack_metadata = opstack_metadata = pd.read_csv('../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "combined_l2b_df['l2beat_slug'] = combined_l2b_df['chain']\n",
    "meta_cols = ['l2beat_slug', 'is_op_chain','mainnet_chain_id','op_based_version', 'alignment','chain_name', 'display_name']\n",
    "\n",
    "l2b_enriched_df = combined_l2b_df.merge(opstack_metadata[meta_cols], on='l2beat_slug', how = 'left')\n",
    "\n",
    "l2b_enriched_df['alignment'] = l2b_enriched_df['alignment'].fillna('Other EVMs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_63216/444546690.py:7: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[column] = df[column].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "boolean_columns = ['is_op_chain', 'is_upcoming', 'is_archived', 'is_current_chain']\n",
    "dfs = [l2b_enriched_df, l2beat_meta]\n",
    "\n",
    "for df in dfs:\n",
    "    for column in boolean_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define aggregation functions for each column\n",
    "aggregations = {\n",
    "    'totalUsd': ['min', 'last', 'mean'], #valueUsd\n",
    "    'transactions': ['sum', 'mean'],\n",
    "    'canonicalUsd': ['min', 'last', 'mean'], #cbvUsd\n",
    "    'externalUsd': ['min', 'last', 'mean'], #ebvUsd\n",
    "    'nativeUsd': ['min', 'last', 'mean'], #nmvUsd\n",
    "}\n",
    "# Function to perform aggregation based on frequency\n",
    "def aggregate_data(df, freq, date_col='timestamp', groupby_cols=None, aggs=None):\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = ['chain', 'chainId', 'layer', 'is_op_chain', 'mainnet_chain_id', 'op_based_version', 'alignment', 'chain_name', 'display_name', 'provider', 'is_upcoming','is_archived','is_current_chain']\n",
    "    if aggs is None:\n",
    "        aggs = aggregations\n",
    "\n",
    "    # Group by the specified frequency and other columns, then apply aggregations\n",
    "    df_agg = df.groupby([pd.Grouper(key=date_col, freq=freq)] + groupby_cols, dropna=False).agg(aggs).reset_index()\n",
    "\n",
    "    # Flatten the hierarchical column index and concatenate aggregation function names with column names\n",
    "    df_agg.columns = ['_'.join(filter(None, col)).rstrip('_') for col in df_agg.columns]\n",
    "\n",
    "    # Rename the 'timestamp' column based on the frequency\n",
    "    date_col_name = 'month' if freq == 'MS' else 'week'\n",
    "    df_agg.rename(columns={date_col: date_col_name}, inplace=True)\n",
    "\n",
    "    # Group by 'chain' and rank the rows within each group based on the date column\n",
    "    df_agg[f'{date_col_name}s_live'] = df_agg.groupby('chain')[date_col_name].rank(method='min')\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "# Perform monthly aggregation\n",
    "l2b_monthly_df = aggregate_data(l2b_enriched_df, freq='MS')\n",
    "# Perform weekly aggregation\n",
    "l2b_weekly_df = aggregate_data(l2b_enriched_df, freq='W-MON')\n",
    "\n",
    "# Sample output\n",
    "# l2b_weekly_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading growthepie_l2_activity\n",
      "Response status code: 200\n",
      "Response content: b'{\"success\":true,\"table_name\":\"growthepie_l2_activity\"}'\n",
      "table at: dune.oplabspbc.dataset_growthepie_l2_activity\n",
      "uploading growthepie_l2_metadata\n",
      "Response status code: 200\n",
      "Response content: b'{\"success\":true,\"table_name\":\"growthepie_l2_metadata\"}'\n",
      "table at: dune.oplabspbc.dataset_growthepie_l2_metadata\n",
      "uploading l2beat_l2_activity\n",
      "Response status code: 200\n",
      "Response content: b'{\"success\":true,\"table_name\":\"l2beat_l2_activity\"}'\n",
      "table at: dune.oplabspbc.dataset_l2beat_l2_activity\n",
      "uploading l2beat_l2_activity_monthly\n",
      "Response status code: 200\n",
      "Response content: b'{\"success\":true,\"table_name\":\"l2beat_l2_activity_monthly\"}'\n",
      "table at: dune.oplabspbc.dataset_l2beat_l2_activity_monthly\n",
      "uploading l2beat_l2_activity_weekly\n",
      "Response status code: 200\n",
      "Response content: b'{\"success\":true,\"table_name\":\"l2beat_l2_activity_weekly\"}'\n",
      "table at: dune.oplabspbc.dataset_l2beat_l2_activity_weekly\n",
      "uploading l2beat_l2_metadata\n",
      "Response status code: 200\n",
      "Response content: b'{\"success\":true,\"table_name\":\"l2beat_l2_metadata\"}'\n",
      "table at: dune.oplabspbc.dataset_l2beat_l2_metadata\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "folder = 'outputs/'\n",
    "combined_gtp_df.to_csv(folder + 'growthepie_l2_activity.csv', index = False)\n",
    "gtp_meta_api.to_csv(folder + 'growthepie_l2_metadata.csv', index = False)\n",
    "l2b_enriched_df.to_csv(folder + 'l2beat_l2_activity.csv', index = False)\n",
    "l2beat_meta.to_csv(folder + 'l2beat_l2_metadata.csv', index = False)\n",
    "l2b_monthly_df.to_csv(folder + 'l2beat_l2_activity_monthly.csv', index = False)\n",
    "l2b_weekly_df.to_csv(folder + 'l2beat_l2_activity_weekly.csv', index = False)\n",
    "l2beat_aoc.to_csv(folder + 'l2beat_aoc_by_token.csv', index = False)\n",
    "# Post to Dune API\n",
    "d.write_dune_api_from_pandas(combined_gtp_df, 'growthepie_l2_activity',\\\n",
    "                             'L2 Usage Activity from GrowThePie')\n",
    "d.write_dune_api_from_pandas(gtp_meta_api, 'growthepie_l2_metadata',\\\n",
    "                             'L2 Metadata from GrowThePie')\n",
    "d.write_dune_api_from_pandas(l2b_enriched_df, 'l2beat_l2_activity',\\\n",
    "                             'L2 Usage Activity from L2Beat')\n",
    "d.write_dune_api_from_pandas(l2b_monthly_df, 'l2beat_l2_activity_monthly',\\\n",
    "                             'Monthly L2 Usage Activity from L2Beat')\n",
    "d.write_dune_api_from_pandas(l2b_weekly_df, 'l2beat_l2_activity_weekly',\\\n",
    "                             'Weekly L2 Usage Activity from L2Beat')\n",
    "d.write_dune_api_from_pandas(l2beat_meta, 'l2beat_l2_metadata',\\\n",
    "                             'L2 Metadata from L2Beat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 14:35:00,792 ERROR google.cloud.bigquery._pandas_helpers Error converting Pandas column with name: \"dt\" and datatype: \"object\" to an appropriate pyarrow datatype: Array, ListArray, or StructArray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Writing api_table_uploads.daily_growthepie_l2_activity\n"
     ]
    },
    {
     "ename": "ArrowTypeError",
     "evalue": "Error converting Pandas column with name: \"dt\" and datatype: \"object\" to an appropriate pyarrow datatype: Array, ListArray, or StructArray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:340\u001b[0m, in \u001b[0;36mbq_to_arrow_array\u001b[0;34m(series, bq_field)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_pandas(series, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39marrow_type)\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyarrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrow_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mArrowTypeError:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/pyarrow/array.pxi:1104\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/pyarrow/array.pxi:345\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/pyarrow/array.pxi:85\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: object of type <class 'str'> cannot be converted to int",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#BQ Upload\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbqu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_df_to_bq_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_gtp_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdaily_growthepie_l2_activity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m bqu\u001b[38;5;241m.\u001b[39mwrite_df_to_bq_table(gtp_meta_api, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrowthepie_l2_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/op-analytics/other_chains_tracking/../helper_functions/google_bq_utils.py:100\u001b[0m, in \u001b[0;36mwrite_df_to_bq_table\u001b[0;34m(df, table_id, dataset_id, write_mode, project_id)\u001b[0m\n\u001b[1;32m     98\u001b[0m client \u001b[38;5;241m=\u001b[39m connect_bq_client(project_id)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Load the DataFrame into BigQuery\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_table_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Wait for the job to complete\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/google/cloud/bigquery/client.py:2793\u001b[0m, in \u001b[0;36mClient.load_table_from_dataframe\u001b[0;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[1;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parquet_compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnappy\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# adjust the default value\u001b[39;00m\n\u001b[1;32m   2791\u001b[0m         parquet_compression \u001b[38;5;241m=\u001b[39m parquet_compression\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m-> 2793\u001b[0m     \u001b[43m_pandas_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe_to_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_job_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtmppath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_use_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2801\u001b[0m     dataframe\u001b[38;5;241m.\u001b[39mto_parquet(\n\u001b[1;32m   2802\u001b[0m         tmppath,\n\u001b[1;32m   2803\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2809\u001b[0m         ),\n\u001b[1;32m   2810\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:686\u001b[0m, in \u001b[0;36mdataframe_to_parquet\u001b[0;34m(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\u001b[0m\n\u001b[1;32m    679\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    680\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_compliant_nested_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: parquet_use_compliant_nested_type}\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _versions_helpers\u001b[38;5;241m.\u001b[39mPYARROW_VERSIONS\u001b[38;5;241m.\u001b[39muse_compliant_nested_type\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    683\u001b[0m )\n\u001b[1;32m    685\u001b[0m bq_schema \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39m_to_schema_fields(bq_schema)\n\u001b[0;32m--> 686\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe_to_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbq_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_table(\n\u001b[1;32m    688\u001b[0m     arrow_table,\n\u001b[1;32m    689\u001b[0m     filepath,\n\u001b[1;32m    690\u001b[0m     compression\u001b[38;5;241m=\u001b[39mparquet_compression,\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    692\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:629\u001b[0m, in \u001b[0;36mdataframe_to_arrow\u001b[0;34m(dataframe, bq_schema)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bq_field \u001b[38;5;129;01min\u001b[39;00m bq_schema:\n\u001b[1;32m    627\u001b[0m     arrow_names\u001b[38;5;241m.\u001b[39mappend(bq_field\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    628\u001b[0m     arrow_arrays\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 629\u001b[0m         \u001b[43mbq_to_arrow_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_column_or_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbq_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbq_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     )\n\u001b[1;32m    631\u001b[0m     arrow_fields\u001b[38;5;241m.\u001b[39mappend(bq_to_arrow_field(bq_field, arrow_arrays[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtype))\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m((field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m arrow_fields)):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:344\u001b[0m, in \u001b[0;36mbq_to_arrow_array\u001b[0;34m(series, bq_field)\u001b[0m\n\u001b[1;32m    342\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mError converting Pandas column with name: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseries\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and datatype: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseries\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to an appropriate pyarrow datatype: Array, ListArray, or StructArray\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    343\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mArrowTypeError(msg)\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: Error converting Pandas column with name: \"dt\" and datatype: \"object\" to an appropriate pyarrow datatype: Array, ListArray, or StructArray"
     ]
    }
   ],
   "source": [
    "#BQ Upload\n",
    "bqu.write_df_to_bq_table(combined_gtp_df, 'daily_growthepie_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(gtp_meta_api, 'growthepie_l2_metadata')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2b_enriched_df, 'daily_l2beat_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2b_monthly_df, 'monthly_l2beat_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2b_weekly_df, 'weekly_l2beat_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2beat_meta, 'l2beat_l2_metadata')\n",
    "time.sleep(1)\n",
    "bqu.append_and_upsert_df_to_bq_table(l2beat_aoc, 'daily_l2beat_aoc_by_token', unique_keys=['dt','chain','token_type','asset_id','chain','address'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
