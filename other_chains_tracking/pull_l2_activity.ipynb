{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start l2 activity')\n",
    "import sys\n",
    "sys.path.append(\"../helper_functions\")\n",
    "import duneapi_utils as d\n",
    "import growthepieapi_utils as gtp\n",
    "import l2beat_utils as ltwo\n",
    "import csv_utils as cu\n",
    "import google_bq_utils as bqu\n",
    "import pandas_utils as pu\n",
    "sys.path.pop()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Usage\n",
    "gtp_api = gtp.get_growthepie_api_data()\n",
    "gtp_meta_api = gtp.get_growthepie_api_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2beat_aop = ltwo.get_daily_aop_by_token()\n",
    "\n",
    "# l2beat_aop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2beat_df = ltwo.get_all_l2beat_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2beat_meta = ltwo.get_l2beat_metadata()\n",
    "l2beat_meta['chain'] = l2beat_meta['slug']\n",
    "l2beat_meta['is_upcoming'] = l2beat_meta['is_upcoming'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2beat_meta[l2beat_meta['slug'].str.contains('zksync')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_l2b_df = l2beat_df.merge(l2beat_meta[\n",
    "        ['chain','name','layer','chainId','provider','provider_entity','category',\\\n",
    "         'is_upcoming','is_archived','is_current_chain']\n",
    "        ], on='chain',how='outer')\n",
    "# combined_l2b_df.tail(5)\n",
    "\n",
    "combined_l2b_df['chainId'] = combined_l2b_df['chainId'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gtp_df = gtp_api.merge(gtp_meta_api[['origin_key','chain_name']], on='origin_key',how='left')\n",
    "combined_gtp_df[\"date\"] = pd.to_datetime(combined_gtp_df[\"date\"], errors='coerce')\n",
    "# combined_gtp_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Columns\n",
    "# Assuming combined_gtp_df is your DataFrame\n",
    "column_names = combined_gtp_df.columns\n",
    "\n",
    "for col in column_names:\n",
    "    if col.endswith('_usd'):\n",
    "        # Construct the new column name by replacing '_usd' with '_eth'\n",
    "        new_col_name = col.replace('_usd', '_eth')\n",
    "        \n",
    "        # Check if the new column name exists in the DataFrame\n",
    "        if new_col_name not in combined_gtp_df.columns:\n",
    "            # If it doesn't exist, create the column and fill it with nan values\n",
    "            combined_gtp_df[new_col_name] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combined_gtp_df.dtypes)\n",
    "# print(l2beat_df.dtypes)\n",
    "# combined_gtp_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Metadata\n",
    "opstack_metadata = opstack_metadata = pd.read_csv('../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "combined_l2b_df['l2beat_slug'] = combined_l2b_df['chain']\n",
    "meta_cols = ['l2beat_slug', 'is_op_chain','mainnet_chain_id','op_based_version', 'alignment','chain_name', 'display_name']\n",
    "\n",
    "l2b_enriched_df = combined_l2b_df.merge(opstack_metadata[meta_cols], on='l2beat_slug', how = 'left')\n",
    "\n",
    "l2b_enriched_df['alignment'] = l2b_enriched_df['alignment'].fillna('Other EVMs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = ['is_op_chain', 'is_upcoming', 'is_archived', 'is_current_chain']\n",
    "dfs = [l2b_enriched_df, l2beat_meta]\n",
    "\n",
    "for df in dfs:\n",
    "    for column in boolean_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define aggregation functions for each column\n",
    "aggregations = {\n",
    "    'totalUsd': ['min', 'last', 'mean'], #valueUsd\n",
    "    'transactions': ['sum', 'mean'],\n",
    "    'canonicalUsd': ['min', 'last', 'mean'], #cbvUsd\n",
    "    'externalUsd': ['min', 'last', 'mean'], #ebvUsd\n",
    "    'nativeUsd': ['min', 'last', 'mean'], #nmvUsd\n",
    "}\n",
    "# Function to perform aggregation based on frequency\n",
    "def aggregate_data(df, freq, date_col='timestamp', groupby_cols=None, aggs=None):\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = ['chain', 'chainId', 'layer', 'is_op_chain', 'mainnet_chain_id', 'op_based_version', 'alignment', 'chain_name', 'display_name', 'provider', 'is_upcoming','is_archived','is_current_chain']\n",
    "    if aggs is None:\n",
    "        aggs = aggregations\n",
    "\n",
    "    # Group by the specified frequency and other columns, then apply aggregations\n",
    "    df_agg = df.groupby([pd.Grouper(key=date_col, freq=freq)] + groupby_cols, dropna=False).agg(aggs).reset_index()\n",
    "\n",
    "    # Flatten the hierarchical column index and concatenate aggregation function names with column names\n",
    "    df_agg.columns = ['_'.join(filter(None, col)).rstrip('_') for col in df_agg.columns]\n",
    "\n",
    "    # Rename the 'timestamp' column based on the frequency\n",
    "    date_col_name = 'month' if freq == 'MS' else 'week'\n",
    "    df_agg.rename(columns={date_col: date_col_name}, inplace=True)\n",
    "\n",
    "    # Group by 'chain' and rank the rows within each group based on the date column\n",
    "    df_agg[f'{date_col_name}s_live'] = df_agg.groupby('chain')[date_col_name].rank(method='min')\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "# Perform monthly aggregation\n",
    "l2b_monthly_df = aggregate_data(l2b_enriched_df, freq='MS')\n",
    "# Perform weekly aggregation\n",
    "l2b_weekly_df = aggregate_data(l2b_enriched_df, freq='W-MON')\n",
    "\n",
    "# Sample output\n",
    "# l2b_weekly_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "folder = 'outputs/'\n",
    "combined_gtp_df.to_csv(folder + 'growthepie_l2_activity.csv', index = False)\n",
    "gtp_meta_api.to_csv(folder + 'growthepie_l2_metadata.csv', index = False)\n",
    "l2b_enriched_df.to_csv(folder + 'l2beat_l2_activity.csv', index = False)\n",
    "l2beat_meta.to_csv(folder + 'l2beat_l2_metadata.csv', index = False)\n",
    "l2b_monthly_df.to_csv(folder + 'l2beat_l2_activity_monthly.csv', index = False)\n",
    "l2b_weekly_df.to_csv(folder + 'l2beat_l2_activity_weekly.csv', index = False)\n",
    "# Post to Dune API\n",
    "d.write_dune_api_from_pandas(combined_gtp_df, 'growthepie_l2_activity',\\\n",
    "                             'L2 Usage Activity from GrowThePie')\n",
    "d.write_dune_api_from_pandas(gtp_meta_api, 'growthepie_l2_metadata',\\\n",
    "                             'L2 Metadata from GrowThePie')\n",
    "d.write_dune_api_from_pandas(l2b_enriched_df, 'l2beat_l2_activity',\\\n",
    "                             'L2 Usage Activity from L2Beat')\n",
    "d.write_dune_api_from_pandas(l2b_monthly_df, 'l2beat_l2_activity_monthly',\\\n",
    "                             'Monthly L2 Usage Activity from L2Beat')\n",
    "d.write_dune_api_from_pandas(l2b_weekly_df, 'l2beat_l2_activity_weekly',\\\n",
    "                             'Weekly L2 Usage Activity from L2Beat')\n",
    "d.write_dune_api_from_pandas(l2beat_meta, 'l2beat_l2_metadata',\\\n",
    "                             'L2 Metadata from L2Beat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2beat_meta_copy = l2beat_meta.copy()\n",
    "\n",
    "# print(l2beat_meta.dtypes)\n",
    "# display(l2beat_meta.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # l2beat_meta\n",
    "\n",
    "# # Check for any flattens to do\n",
    "# def mock_flatten_nested_data(df, column_name):\n",
    "#     # Check if the column contains dictionaries or arrays\n",
    "#     if df[column_name].apply(lambda x: isinstance(x, dict) or isinstance(x, list)).any():\n",
    "#         flat_df = pd.json_normalize(df[column_name].apply(lambda x: x if isinstance(x, dict) else {}))\n",
    "#         flat_df = flat_df.add_prefix(f\"{column_name}_\")\n",
    "#         return pd.concat([df.drop(column_name, axis=1), flat_df], axis=1)\n",
    "#     else:\n",
    "#         return df\n",
    "\n",
    "# for column_name, column_type in l2beat_meta_copy.dtypes.items():\n",
    "#         if column_type == 'object':\n",
    "#                 # Attempt to flatten nested data if the column contains arrays or dictionaries\n",
    "#                 try:\n",
    "#                         l2beat_meta_copy = mock_flatten_nested_data(l2beat_meta_copy, column_name)\n",
    "#                         continue  # Skip adding the original column to the schema\n",
    "#                 except ValueError:\n",
    "#                         l2beat_meta_copy = l2beat_meta_copy\n",
    "#                         continue\n",
    "\n",
    "# # display(l2beat_meta_copy.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2b_enriched_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BQ Upload\n",
    "bqu.write_df_to_bq_table(combined_gtp_df, 'daily_growthepie_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(gtp_meta_api, 'growthepie_l2_metadata')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2b_enriched_df, 'daily_l2beat_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2b_monthly_df, 'monthly_l2beat_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2b_weekly_df, 'weekly_l2beat_l2_activity')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(l2beat_meta, 'l2beat_l2_metadata')\n",
    "time.sleep(1)\n",
    "bqu.append_and_upsert_df_to_bq_table(l2beat_aop, 'daily_l2beat_aop_by_token', unique_keys=['dt','project','token_type','asset_id','chain','address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bqu.delete_bq_table('api_table_uploads','daily_growthepie_l2_activity')\n",
    "# bqu.delete_bq_table('api_table_uploads','growthepie_l2_metadata')\n",
    "# bqu.delete_bq_table('api_table_uploads','daily_l2beat_l2_activity')\n",
    "# bqu.delete_bq_table('api_table_uploads','monthly_l2beat_l2_activity')\n",
    "# bqu.delete_bq_table('api_table_uploads','weekly_l2beat_l2_activity')\n",
    "# bqu.delete_bq_table('api_table_uploads','l2beat_l2_metadata')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
